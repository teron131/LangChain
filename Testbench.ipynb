{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teron/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import opencc\n",
    "from dotenv import load_dotenv\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "from langchain_openai.chat_models.base import ChatOpenAI\n",
    "from langchain_together.llms import Together\n",
    "from langchain_core.messages.base import get_msg_title_repr\n",
    "import gradio as gr\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ChatOpenRouter(ChatOpenAI):\n",
    "    openai_api_base: str\n",
    "    openai_api_key: str\n",
    "    model_name: str\n",
    "\n",
    "    def __init__(self, model_name: str, openai_api_key: Optional[str] = None, openai_api_base: str = \"https://openrouter.ai/api/v1\", **kwargs):\n",
    "        openai_api_key = openai_api_key or os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        super().__init__(openai_api_base=openai_api_base, openai_api_key=openai_api_key, model_name=model_name, **kwargs)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a science communicator specializing in astronomy. Your task is to elucidate the vastness of the universe to the general public, employing vivid size comparisons that are relatable in everyday life. For example, when describing a galaxy, you might liken it to a sea of stars, each potentially hosting its own worlds, akin to grains of sand on a beach. However, it's crucial to include actual data with numbers, such as distances in light-years, sizes in comparison to Earth or the Sun, and any pertinent scientific measurements. Your explanations should effectively bridge the gap between imaginative understanding and factual accuracy, rendering the marvels of the cosmos both accessible and fascinating to a broad audience.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Translate to Traditional Chinese\n",
    "def s2hk(content):\n",
    "    converter = opencc.OpenCC(\"s2hk\")\n",
    "    return converter.convert(content)\n",
    "\n",
    "\n",
    "def get_answer(question, system_prompt=system_prompt, user_prompt=user_prompt, show_info=False, **kwargs):\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            HumanMessagePromptTemplate.from_template(user_prompt),\n",
    "        ]\n",
    "    )\n",
    "    if show_info:\n",
    "        prompt.pretty_print()\n",
    "\n",
    "    # Use kwargs to override default parameters if provided\n",
    "    model_params = {\n",
    "        \"model_name\": \"openai/gpt-4o\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    "    model_params.update(kwargs)\n",
    "\n",
    "    model = ChatOpenRouter(**model_params)\n",
    "    # model = ChatOpenAI(**model_params)\n",
    "    # model = AzureChatOpenAI(**model_params)\n",
    "    # model = Together(**model_params)\n",
    "\n",
    "    # model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7, max_tokens=4096)\n",
    "    # model = AzureChatOpenAI(model=\"gpt-4o\", temperature=0.7, max_tokens=4096)\n",
    "    # model = Together(model=\"mistralai/Mixtral-8x22B-Instruct-v0.1\", temperature=0.7, max_tokens=4096)\n",
    "    # model = Together(model=\"meta-llama/Meta-Llama-3-70B\", temperature=0.7, max_tokens=4096)\n",
    "    # model = Together(model=\"deepseek-ai/deepseek-coder-33b-instruct\", temperature=0.7, max_tokens=4096)\n",
    "\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=RunnableLambda(\n",
    "                memory.load_memory_variables,\n",
    "            )\n",
    "            | itemgetter(\"chat_history\")\n",
    "        )\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "        | RunnableLambda(s2hk)\n",
    "    )\n",
    "\n",
    "    # Display callback and response\n",
    "    with get_openai_callback() as callback:\n",
    "        response = chain.invoke({\"question\": question})\n",
    "        if show_info:\n",
    "            print(get_msg_title_repr(\"Callback\", bold=True), end=\"\\n\\n\")\n",
    "            print(callback, end=\"\\n\\n\")\n",
    "            print(get_msg_title_repr(\"Response\", bold=True), end=\"\\n\\n\")\n",
    "        print(response)\n",
    "\n",
    "    memory.save_context({\"question\": question}, {\"response\": response})\n",
    "\n",
    "    return prompt, response\n",
    "\n",
    "\n",
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def gradio_chat_interface():\n",
    "    def wrapped_get_answer(input_message, chat_history, system_prompt):\n",
    "        # Extract the last question from the chat history\n",
    "        question = input_message\n",
    "        # Call the get_answer function\n",
    "        _, response = get_answer(question, system_prompt, user_prompt)\n",
    "        return response\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        chatbot = gr.Chatbot()\n",
    "        system_prompt_input = gr.Textbox(lines=5, placeholder=\"Enter the system prompt here...\", label=\"System Prompt\")\n",
    "        message = gr.Textbox(placeholder=\"Enter your message here...\")\n",
    "\n",
    "        def respond(message, chat_history, system_prompt):\n",
    "            response = wrapped_get_answer(message, chat_history, system_prompt)\n",
    "            chat_history.append([message, response])\n",
    "            return chat_history, \"\"\n",
    "\n",
    "        message.submit(respond, [message, chatbot, system_prompt_input], [chatbot, message])\n",
    "\n",
    "    demo.launch()\n",
    "\n",
    "\n",
    "# Call the helper function to launch the interface\n",
    "gradio_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"Explain gradient descent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent is an optimization algorithm commonly used in machine learning and deep learning for training models. The main idea behind gradient descent is to minimize a cost function (also known as a loss function) by iteratively adjusting the model's parameters in the direction that reduces the cost.\n",
      "\n",
      "Hereâ€™s a step-by-step breakdown of how gradient descent works:\n",
      "\n",
      "1. **Initialization**: Start by initializing the model parameters (weights and biases) with some initial values. These can be random values or zeros.\n",
      "\n",
      "2. **Compute the Cost Function**: Calculate the cost function, which measures how well the model's predictions match the actual target values. Common cost functions include Mean Squared Error (MSE) for regression problems and Cross-Entropy Loss for classification problems.\n",
      "\n",
      "3. **Compute the Gradient**: Compute the gradient of the cost function with respect to each model parameter. The gradient is a vector of partial derivatives, indicating the direction and rate of the steepest increase in the cost function.\n",
      "\n",
      "4. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient. The size of the adjustment is controlled by the learning rate, a hyperparameter that you must choose carefully. Mathematically, this can be expressed as:\n",
      "   \\[\n",
      "   \\theta_{new} = \\theta_{old} - \\alpha \\nabla J(\\theta)\n",
      "   \\]\n",
      "   where \\(\\theta\\) represents the model parameters, \\(\\alpha\\) is the learning rate, and \\(\\nabla J(\\theta)\\) is the gradient of the cost function with respect to the parameters.\n",
      "\n",
      "5. **Iterate**: Repeat steps 2-4 until the cost function converges to a minimum value, which means that further adjustments to the parameters do not significantly reduce the cost. Convergence criteria can be defined by a fixed number of iterations, a threshold for the cost function, or the magnitude of the gradient.\n",
      "\n",
      "### Variants of Gradient Descent\n",
      "\n",
      "1. **Batch Gradient Descent**: Uses the entire dataset to compute the gradient at each step. This can be computationally expensive for large datasets.\n",
      "\n",
      "2. **Stochastic Gradient Descent (SGD)**: Uses only a single training example to compute the gradient at each step. This makes the algorithm faster but introduces more noise into the parameter updates.\n",
      "\n",
      "3. **Mini-Batch Gradient Descent**: A compromise between Batch Gradient Descent and SGD. It uses a small batch of training examples to compute the gradient at each step. This approach is widely used as it balances the speed and stability of parameter updates.\n",
      "\n",
      "### Challenges and Solutions\n",
      "\n",
      "- **Choosing the Learning Rate**: If the learning rate is too high, the algorithm may overshoot the minimum and fail to converge. If it's too low, the algorithm will converge very slowly. Techniques such as learning rate schedules and adaptive learning rates (e.g., using algorithms like Adam, RMSprop, or Adagrad) can help.\n",
      "\n",
      "- **Local Minima and Saddle Points**: The cost function may have multiple local minima or saddle points. Stochastic methods like SGD can help the algorithm escape these traps due to their inherent noise.\n",
      "\n",
      "- **Feature Scaling**: Features should be scaled (e.g., using standardization or normalization) to ensure that the gradient descent algorithm converges more efficiently.\n",
      "\n",
      "Gradient descent is a fundamental technique in machine learning and optimization, forming the backbone of many algorithms used in practice.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='\\n{question}\\n'))]),\n",
       " \"Gradient descent is an optimization algorithm commonly used in machine learning and deep learning for training models. The main idea behind gradient descent is to minimize a cost function (also known as a loss function) by iteratively adjusting the model's parameters in the direction that reduces the cost.\\n\\nHereâ€™s a step-by-step breakdown of how gradient descent works:\\n\\n1. **Initialization**: Start by initializing the model parameters (weights and biases) with some initial values. These can be random values or zeros.\\n\\n2. **Compute the Cost Function**: Calculate the cost function, which measures how well the model's predictions match the actual target values. Common cost functions include Mean Squared Error (MSE) for regression problems and Cross-Entropy Loss for classification problems.\\n\\n3. **Compute the Gradient**: Compute the gradient of the cost function with respect to each model parameter. The gradient is a vector of partial derivatives, indicating the direction and rate of the steepest increase in the cost function.\\n\\n4. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient. The size of the adjustment is controlled by the learning rate, a hyperparameter that you must choose carefully. Mathematically, this can be expressed as:\\n   \\\\[\\n   \\\\theta_{new} = \\\\theta_{old} - \\\\alpha \\\\nabla J(\\\\theta)\\n   \\\\]\\n   where \\\\(\\\\theta\\\\) represents the model parameters, \\\\(\\\\alpha\\\\) is the learning rate, and \\\\(\\\\nabla J(\\\\theta)\\\\) is the gradient of the cost function with respect to the parameters.\\n\\n5. **Iterate**: Repeat steps 2-4 until the cost function converges to a minimum value, which means that further adjustments to the parameters do not significantly reduce the cost. Convergence criteria can be defined by a fixed number of iterations, a threshold for the cost function, or the magnitude of the gradient.\\n\\n### Variants of Gradient Descent\\n\\n1. **Batch Gradient Descent**: Uses the entire dataset to compute the gradient at each step. This can be computationally expensive for large datasets.\\n\\n2. **Stochastic Gradient Descent (SGD)**: Uses only a single training example to compute the gradient at each step. This makes the algorithm faster but introduces more noise into the parameter updates.\\n\\n3. **Mini-Batch Gradient Descent**: A compromise between Batch Gradient Descent and SGD. It uses a small batch of training examples to compute the gradient at each step. This approach is widely used as it balances the speed and stability of parameter updates.\\n\\n### Challenges and Solutions\\n\\n- **Choosing the Learning Rate**: If the learning rate is too high, the algorithm may overshoot the minimum and fail to converge. If it's too low, the algorithm will converge very slowly. Techniques such as learning rate schedules and adaptive learning rates (e.g., using algorithms like Adam, RMSprop, or Adagrad) can help.\\n\\n- **Local Minima and Saddle Points**: The cost function may have multiple local minima or saddle points. Stochastic methods like SGD can help the algorithm escape these traps due to their inherent noise.\\n\\n- **Feature Scaling**: Features should be scaled (e.g., using standardization or normalization) to ensure that the gradient descent algorithm converges more efficiently.\\n\\nGradient descent is a fundamental technique in machine learning and optimization, forming the backbone of many algorithms used in practice.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(question, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Describe Sagittarius A* and TON 618.'),\n",
       "  AIMessage(content=\"Sagittarius A* and TON 618 are both fascinating celestial objects that showcase the incredible diversity and scale of the universe.\\n\\nLet's start with Sagittarius A*, which is a supermassive black hole located at the center of our Milky Way galaxy, approximately 26,000 light-years away from Earth. To put this distance into perspective, if Earth were the size of a pea, Sagittarius A* would be about 1,000 kilometers away, roughly the distance between New York City and Chicago.\\n\\nNow, let's delve into the mind-boggling characteristics of Sagittarius A*. This supermassive black hole has a mass equivalent to about 4 million times that of our Sun, compressed into a region smaller than the size of our solar system. Imagine packing the entire mass of 4 million Suns into an area smaller than the orbit of Neptune â€“ that's the density and gravitational pull of Sagittarius A*.\\n\\nMoving on to TON 618, we shift our focus to a distant quasar located around 10.4 billion light-years away from Earth. This means that the light we see from TON 618 today actually started its journey towards us when the universe was much younger.\\n\\nTON 618 is one of the most massive black holes known, with a mass estimated to be around 66 billion times that of our Sun. To envision the sheer enormity of TON 618, imagine a black hole so massive that it would easily engulf the entire orbit of Neptune, our Solar System's outermost planet.\\n\\nIn summary, Sagittarius A* and TON 618 exemplify the extremes of the universe â€“ from the supermassive black hole at the heart of our galaxy to the colossal black hole residing billions of light-years away. These celestial giants serve as a reminder of the awe-inspiring vastness and diversity of the cosmos, inviting us to ponder the mysteries that lie beyond our earthly confines.\")]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nYou are a science communicator specializing in astronomy. Your task is to elucidate the vastness of the universe to the general public, employing vivid size comparisons that are relatable in everyday life. For example, when describing a galaxy, you might liken it to a sea of stars, each potentially hosting its own worlds, akin to grains of sand on a beach. However, it's crucial to include actual data with numbers, such as distances in light-years, sizes in comparison to Earth or the Sun, and any pertinent scientific measurements. Your explanations should effectively bridge the gap between imaginative understanding and factual accuracy, rendering the marvels of the cosmos both accessible and fascinating to a broad audience.\\n\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='\\n{question}\\n'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
