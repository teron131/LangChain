{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_message = \"Hello\"\n",
        "messages = [(\"user\", user_message)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teron/miniconda3/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found meta/llama-3.3-70b-instruct in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "375d3a0ab9f6402b8abbd0c5ae57f3b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_together import ChatTogether\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Cloud models\n",
        "nim_model = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\")\n",
        "groq_model_fast = ChatGroq(model=\"llama-3.3-70b-specdec\")\n",
        "groq_model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "together_model = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\")\n",
        "openrouter_model = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "# Local models\n",
        "ollama_model = ChatOllama(model=\"llama3.3:70b\")\n",
        "\n",
        "\n",
        "model_id = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "huggingface_model = ChatHuggingFace(llm=hf_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>provider</th>\n",
              "      <th>model</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>completion_tokens</th>\n",
              "      <th>completion_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [provider, model, input, output, completion_tokens, completion_time]\n",
              "Index: []"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"llm_comparison.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "'(' was never closed (2098190050.py, line 61)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 61\u001b[0;36m\u001b[0m\n\u001b[0;31m    df.loc[len(df)] = dict(\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "response = nim_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"NVIDIA\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": response.response_metadata[\"token_usage\"][\"completion_tokens\"],\n",
        "    \"completion_time\": end_time - start_time,\n",
        "}\n",
        "\n",
        "response = groq_model_fast.invoke(messages)\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"Groq\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": response.response_metadata[\"token_usage\"][\"completion_tokens\"],\n",
        "    \"completion_time\": response.response_metadata[\"token_usage\"][\"completion_time\"],\n",
        "}\n",
        "\n",
        "response = groq_model.invoke(messages)\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"Groq\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": response.response_metadata[\"token_usage\"][\"completion_tokens\"],\n",
        "    \"completion_time\": response.response_metadata[\"token_usage\"][\"completion_time\"],\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "response = together_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"Together\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": response.response_metadata[\"token_usage\"][\"completion_tokens\"],\n",
        "    \"completion_time\": end_time - start_time,\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "response = openrouter_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"OpenRouter\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": response.response_metadata[\"token_usage\"][\"completion_tokens\"],\n",
        "    \"completion_time\": end_time - start_time,\n",
        "}\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = ollama_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"Ollama\",\n",
        "    \"model\": response.response_metadata[\"model\"],\n",
        "    \"type\": \"local\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": response.response_metadata[\"eval_count\"],\n",
        "    \"completion_time\": response.response_metadata[\"total_duration\"] / 10**9,\n",
        "}\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "df.loc[len(df)] = {\n",
        "    \"provider\": \"HuggingFace\",\n",
        "    \"model\": model_id,\n",
        "    \"type\": \"local\",\n",
        "    \"input\": messages,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": len(tokenizer.encode(response.content)),\n",
        "    \"completion_time\": end_time - start_time,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose parallel processing was divine,\n",
            "It crunched with great zest,\n",
            "Through its cores, it did quest,\n",
            "For solutions to problems of design.\n"
          ]
        }
      ],
      "source": [
        "response = nim_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose parallel processing was divine,\\nIt crunched with great zest,\\nThrough its cores, it did quest,\\nFor solutions to problems of design.' additional_kwargs={} response_metadata={'role': 'assistant', 'content': 'There once was a GPU so fine,\\nWhose parallel processing was divine,\\nIt crunched with great zest,\\nThrough its cores, it did quest,\\nFor solutions to problems of design.', 'token_usage': {'prompt_tokens': 22, 'total_tokens': 59, 'completion_tokens': 37}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.3-70b-instruct'} id='run-71e9f704-c32e-4bf9-b2f1-159cc66aacb7-0' usage_metadata={'input_tokens': 22, 'output_tokens': 37, 'total_tokens': 59} role='assistant'\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Introduction to Gradient Descent**\n",
            "=====================================\n",
            "\n",
            "Gradient descent is a fundamental concept in machine learning and optimization, used to minimize the loss function of a model by iteratively adjusting its parameters. It is a widely used algorithm in various fields, including deep learning, natural language processing, and computer vision. In this explanation, we will delve into the details of gradient descent, its types, and its applications.\n",
            "\n",
            "**What is Gradient Descent?**\n",
            "---------------------------\n",
            "\n",
            "Gradient descent is an optimization algorithm that aims to find the minimum of a function by iteratively moving in the direction of the negative gradient. The gradient of a function represents the rate of change of the function with respect to its input variables. By moving in the direction of the negative gradient, we can minimize the function.\n",
            "\n",
            "To understand this concept, let's consider a simple example. Imagine you are standing on a hill, and you want to reach the bottom. The hill represents the function, and the direction of the negative gradient represents the direction of the steepest descent. By following the direction of the negative gradient, you can reach the bottom of the hill, which represents the minimum of the function.\n",
            "\n",
            "**How Gradient Descent Works**\n",
            "-----------------------------\n",
            "\n",
            "The gradient descent algorithm works as follows:\n",
            "\n",
            "1. **Initialize the parameters**: Initialize the parameters of the model, such as the weights and biases of a neural network.\n",
            "2. **Compute the loss**: Compute the loss function of the model, which measures the difference between the predicted output and the actual output.\n",
            "3. **Compute the gradient**: Compute the gradient of the loss function with respect to the parameters of the model.\n",
            "4. **Update the parameters**: Update the parameters of the model by moving in the direction of the negative gradient.\n",
            "5. **Repeat**: Repeat steps 2-4 until convergence or a stopping criterion is reached.\n",
            "\n",
            "**Types of Gradient Descent**\n",
            "-----------------------------\n",
            "\n",
            "There are several types of gradient descent algorithms, including:\n",
            "\n",
            "* **Batch Gradient Descent**: This type of gradient descent uses the entire dataset to compute the gradient of the loss function. It is computationally expensive and may not be suitable for large datasets.\n",
            "* **Stochastic Gradient Descent (SGD)**: This type of gradient descent uses a single example from the dataset to compute the gradient of the loss function. It is computationally efficient and suitable for large datasets.\n",
            "* **Mini-Batch Gradient Descent**: This type of gradient descent uses a small batch of examples from the dataset to compute the gradient of the loss function. It is a compromise between batch gradient descent and stochastic gradient descent.\n",
            "* **Momentum Gradient Descent**: This type of gradient descent adds a momentum term to the update rule, which helps to escape local minima and converge to the global minimum.\n",
            "* **Nesterov Accelerated Gradient Descent**: This type of gradient descent uses a combination of momentum and acceleration to converge to the global minimum.\n",
            "\n",
            "**Advantages and Disadvantages of Gradient Descent**\n",
            "---------------------------------------------------\n",
            "\n",
            "The advantages of gradient descent include:\n",
            "\n",
            "* **Simple to implement**: Gradient descent is a simple algorithm to implement, and it can be used with various types of models.\n",
            "* **Fast convergence**: Gradient descent can converge quickly to the minimum of the loss function, especially when the learning rate is properly tuned.\n",
            "* **Robust to noise**: Gradient descent is robust to noise in the data, and it can handle large datasets.\n",
            "\n",
            "The disadvantages of gradient descent include:\n",
            "\n",
            "* **Sensitive to learning rate**: Gradient descent is sensitive to the learning rate, and a high learning rate can cause the algorithm to diverge.\n",
            "* **May get stuck in local minima**: Gradient descent may get stuck in local minima, especially when the loss function is non-convex.\n",
            "* **Computationally expensive**: Gradient descent can be computationally expensive, especially when using batch gradient descent or mini-batch gradient descent.\n",
            "\n",
            "**Applications of Gradient Descent**\n",
            "-----------------------------------\n",
            "\n",
            "Gradient descent has numerous applications in machine learning and optimization, including:\n",
            "\n",
            "* **Neural networks**: Gradient descent is used to train neural networks, including deep neural networks.\n",
            "* **Linear regression**: Gradient descent is used to solve linear regression problems.\n",
            "* **Logistic regression**: Gradient descent is used to solve logistic regression problems.\n",
            "* **Support vector machines**: Gradient descent is used to solve support vector machine problems.\n",
            "* **Optimization problems**: Gradient descent is used to solve optimization problems, such as minimizing the loss function of a model.\n",
            "\n",
            "**Conclusion**\n",
            "--------------\n",
            "\n",
            "In conclusion, gradient descent is a fundamental concept in machine learning and optimization, used to minimize the loss function of a model by iteratively adjusting its parameters. It is a widely used algorithm in various fields, including deep learning, natural language processing, and computer vision. The advantages of gradient descent include its simplicity, fast convergence, and robustness to noise. However, it is sensitive to the learning rate and may get stuck in local minima. By understanding the types gradient descent and its types, we can apply it to various problems and achieve better results.\n",
            "\n",
            "**Example Code**\n",
            "---------------\n",
            "\n",
            "Here is an example code in Python using the NumPy library to implement gradient descent:\n",
            "```python\n",
            "import numpy as np\n",
            "\n",
            "# Define the loss function\n",
            "def loss_function(x, y, w, b):\n",
            "    return np.mean((x * w + b - y) ** 2)\n",
            "\n",
            "# Define the gradient of the loss function\n",
            "def gradient(x, y, w, b):\n",
            "    dw = 2 * np.mean(x * (x * w + b - y))\n",
            "    db = 2 * np.mean(x * w + b - y)\n",
            "    return dw, db\n",
            "\n",
            "# Initialize the parameters\n",
            "w = 0.0\n",
            "b = 0.0\n",
            "\n",
            "# Define the learning rate\n",
            "learning_rate = 0.01\n",
            "\n",
            "# Define the number of iterations\n",
            "num_iterations = 1000\n",
            "\n",
            "# Define the dataset\n",
            "x = np.array([1, 2, 3, 4, 5])\n",
            "y = np.array([2, 3, 5, 7, 11])\n",
            "\n",
            "# Train the model\n",
            "for i in range(num_iterations):\n",
            "    # Compute the loss\n",
            "    loss = loss_function(x, y, w, b)\n",
            "    \n",
            "    # Compute the gradient\n",
            "    dw, db = gradient(x, y, w, b)\n",
            "    \n",
            "    # Update the parameters\n",
            "    w -= learning_rate * dw\n",
            "    b -= learning_rate * db\n",
            "    \n",
            "    # Print the loss\n",
            "    print(f\"Iteration {i+1}, Loss: {loss}\")\n",
            "\n",
            "# Print the final parameters\n",
            "print(f\"Final parameters: w = {w}, b = {b}\")\n",
            "```\n",
            "This code implements a simple linear regression model using gradient descent to minimize the mean squared error loss function. The `loss_function` computes the loss, the `gradient` computes the gradient of the loss function, and the `train` loop updates the parameters using gradient descent.\n"
          ]
        }
      ],
      "source": [
        "response = groq_model_fast.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='**Introduction to Gradient Descent**\\n=====================================\\n\\nGradient descent is a fundamental concept in machine learning and optimization, used to minimize the loss function of a model by iteratively adjusting its parameters. It is a widely used algorithm in various fields, including deep learning, natural language processing, and computer vision. In this explanation, we will delve into the details of gradient descent, its types, and its applications.\\n\\n**What is Gradient Descent?**\\n---------------------------\\n\\nGradient descent is an optimization algorithm that aims to find the minimum of a function by iteratively moving in the direction of the negative gradient. The gradient of a function represents the rate of change of the function with respect to its input variables. By moving in the direction of the negative gradient, we can minimize the function.\\n\\nTo understand this concept, let\\'s consider a simple example. Imagine you are standing on a hill, and you want to reach the bottom. The hill represents the function, and the direction of the negative gradient represents the direction of the steepest descent. By following the direction of the negative gradient, you can reach the bottom of the hill, which represents the minimum of the function.\\n\\n**How Gradient Descent Works**\\n-----------------------------\\n\\nThe gradient descent algorithm works as follows:\\n\\n1. **Initialize the parameters**: Initialize the parameters of the model, such as the weights and biases of a neural network.\\n2. **Compute the loss**: Compute the loss function of the model, which measures the difference between the predicted output and the actual output.\\n3. **Compute the gradient**: Compute the gradient of the loss function with respect to the parameters of the model.\\n4. **Update the parameters**: Update the parameters of the model by moving in the direction of the negative gradient.\\n5. **Repeat**: Repeat steps 2-4 until convergence or a stopping criterion is reached.\\n\\n**Types of Gradient Descent**\\n-----------------------------\\n\\nThere are several types of gradient descent algorithms, including:\\n\\n* **Batch Gradient Descent**: This type of gradient descent uses the entire dataset to compute the gradient of the loss function. It is computationally expensive and may not be suitable for large datasets.\\n* **Stochastic Gradient Descent (SGD)**: This type of gradient descent uses a single example from the dataset to compute the gradient of the loss function. It is computationally efficient and suitable for large datasets.\\n* **Mini-Batch Gradient Descent**: This type of gradient descent uses a small batch of examples from the dataset to compute the gradient of the loss function. It is a compromise between batch gradient descent and stochastic gradient descent.\\n* **Momentum Gradient Descent**: This type of gradient descent adds a momentum term to the update rule, which helps to escape local minima and converge to the global minimum.\\n* **Nesterov Accelerated Gradient Descent**: This type of gradient descent uses a combination of momentum and acceleration to converge to the global minimum.\\n\\n**Advantages and Disadvantages of Gradient Descent**\\n---------------------------------------------------\\n\\nThe advantages of gradient descent include:\\n\\n* **Simple to implement**: Gradient descent is a simple algorithm to implement, and it can be used with various types of models.\\n* **Fast convergence**: Gradient descent can converge quickly to the minimum of the loss function, especially when the learning rate is properly tuned.\\n* **Robust to noise**: Gradient descent is robust to noise in the data, and it can handle large datasets.\\n\\nThe disadvantages of gradient descent include:\\n\\n* **Sensitive to learning rate**: Gradient descent is sensitive to the learning rate, and a high learning rate can cause the algorithm to diverge.\\n* **May get stuck in local minima**: Gradient descent may get stuck in local minima, especially when the loss function is non-convex.\\n* **Computationally expensive**: Gradient descent can be computationally expensive, especially when using batch gradient descent or mini-batch gradient descent.\\n\\n**Applications of Gradient Descent**\\n-----------------------------------\\n\\nGradient descent has numerous applications in machine learning and optimization, including:\\n\\n* **Neural networks**: Gradient descent is used to train neural networks, including deep neural networks.\\n* **Linear regression**: Gradient descent is used to solve linear regression problems.\\n* **Logistic regression**: Gradient descent is used to solve logistic regression problems.\\n* **Support vector machines**: Gradient descent is used to solve support vector machine problems.\\n* **Optimization problems**: Gradient descent is used to solve optimization problems, such as minimizing the loss function of a model.\\n\\n**Conclusion**\\n--------------\\n\\nIn conclusion, gradient descent is a fundamental concept in machine learning and optimization, used to minimize the loss function of a model by iteratively adjusting its parameters. It is a widely used algorithm in various fields, including deep learning, natural language processing, and computer vision. The advantages of gradient descent include its simplicity, fast convergence, and robustness to noise. However, it is sensitive to the learning rate and may get stuck in local minima. By understanding the types gradient descent and its types, we can apply it to various problems and achieve better results.\\n\\n**Example Code**\\n---------------\\n\\nHere is an example code in Python using the NumPy library to implement gradient descent:\\n```python\\nimport numpy as np\\n\\n# Define the loss function\\ndef loss_function(x, y, w, b):\\n    return np.mean((x * w + b - y) ** 2)\\n\\n# Define the gradient of the loss function\\ndef gradient(x, y, w, b):\\n    dw = 2 * np.mean(x * (x * w + b - y))\\n    db = 2 * np.mean(x * w + b - y)\\n    return dw, db\\n\\n# Initialize the parameters\\nw = 0.0\\nb = 0.0\\n\\n# Define the learning rate\\nlearning_rate = 0.01\\n\\n# Define the number of iterations\\nnum_iterations = 1000\\n\\n# Define the dataset\\nx = np.array([1, 2, 3, 4, 5])\\ny = np.array([2, 3, 5, 7, 11])\\n\\n# Train the model\\nfor i in range(num_iterations):\\n    # Compute the loss\\n    loss = loss_function(x, y, w, b)\\n    \\n    # Compute the gradient\\n    dw, db = gradient(x, y, w, b)\\n    \\n    # Update the parameters\\n    w -= learning_rate * dw\\n    b -= learning_rate * db\\n    \\n    # Print the loss\\n    print(f\"Iteration {i+1}, Loss: {loss}\")\\n\\n# Print the final parameters\\nprint(f\"Final parameters: w = {w}, b = {b}\")\\n```\\nThis code implements a simple linear regression model using gradient descent to minimize the mean squared error loss function. The `loss_function` computes the loss, the `gradient` computes the gradient of the loss function, and the `train` loop updates the parameters using gradient descent.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1396, 'prompt_tokens': 44, 'total_tokens': 1440, 'completion_time': 0.716620178, 'prompt_time': 0.005324011, 'queue_time': 0.012757349000000001, 'total_time': 0.721944189}, 'model_name': 'llama-3.3-70b-specdec', 'system_fingerprint': 'fp_9eb2d06c09', 'finish_reason': 'stop', 'logprobs': None} id='run-2bb04de0-394a-480f-ae2d-21eb42dd5d87-0' usage_metadata={'input_tokens': 44, 'output_tokens': 1396, 'total_tokens': 1440}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once were GPUs so fine,\n",
            "Whose parallel processing did shine.\n",
            "They crunched with great speed,\n",
            "Through data with ease and deed,\n",
            "And made complex tasks truly divine.\n"
          ]
        }
      ],
      "source": [
        "response = groq_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once were GPUs so fine,\\nWhose parallel processing did shine.\\nThey crunched with great speed,\\nThrough data with ease and deed,\\nAnd made complex tasks truly divine.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83, 'completion_time': 0.130909091, 'prompt_time': 0.008215492, 'queue_time': 0.013799417999999999, 'total_time': 0.139124583}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c0cfa69934', 'finish_reason': 'stop', 'logprobs': None} id='run-7bb45ead-bd14-4ce4-8385-1b148ac737c3-0' usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose computing powers did shine.\n",
            "It processed with speed,\n",
            "And its parallel deed,\n",
            " Made complex tasks truly divine.\n"
          ]
        }
      ],
      "source": [
        "response = together_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did shine.\\nIt processed with speed,\\nAnd its parallel deed,\\n Made complex tasks truly divine.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 47, 'total_tokens': 79, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None} id='run-6be594c7-cb6c-4277-9417-84c03813695e-0' usage_metadata={'input_tokens': 47, 'output_tokens': 32, 'total_tokens': 79, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose computing powers did shine.\n",
            "It processed with speed,\n",
            "And its cores did proceed,\n",
            "To solve complex tasks in no time!\n"
          ]
        }
      ],
      "source": [
        "response = openrouter_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did shine.\\nIt processed with speed,\\nAnd its cores did proceed,\\nTo solve complex tasks in no time!' additional_kwargs={'refusal': ''} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 22, 'total_tokens': 56, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/llama-3.3-70b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9a0086a5-dafa-46e8-bfdf-605270fc720a-0' usage_metadata={'input_tokens': 22, 'output_tokens': 34, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose parallel processing did shine.\n",
            "It crunched with great pace,\n",
            "And a wonderful face,\n",
            "And made computations divine.\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "response = ollama_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose parallel processing did shine.\\nIt crunched with great pace,\\nAnd a wonderful face,\\nAnd made computations divine.' additional_kwargs={} response_metadata={'model': 'llama3.3:70b', 'created_at': '2024-12-13T08:55:46.60147788Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2811016880, 'load_duration': 40834609, 'prompt_eval_count': 22, 'prompt_eval_duration': 87273000, 'eval_count': 33, 'eval_duration': 2640163000} id='run-d08c1855-1daa-4e40-9006-f7e150e0f2d2-0' usage_metadata={'input_tokens': 22, 'output_tokens': 33, 'total_tokens': 55}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 16.20 seconds\n",
            "There once was a GPU so fine,\n",
            "Whose computing powers did truly shine.\n",
            "It processed with zest,\n",
            "And its speeds were the best,\n",
            "And its parallel tasks did align.\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did truly shine.\\nIt processed with zest,\\nAnd its speeds were the best,\\nAnd its parallel tasks did align.' additional_kwargs={} response_metadata={} id='run-c6c15db5-d113-4f40-88d9-5b72c1e00fce-0'\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
