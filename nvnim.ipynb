{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_message = \"\"\"\n",
        "Write a biography of Elon Musk across different time periods and fields he involed the most significantly.\n",
        "\"\"\"\n",
        "\n",
        "messages = [(\"user\", user_message)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teron/miniconda3/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found meta/llama-3.3-70b-instruct in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_together import ChatTogether\n",
        "\n",
        "# Cloud models\n",
        "nim_model = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\")\n",
        "groq_model_fast = ChatGroq(model=\"llama-3.3-70b-specdec\")\n",
        "groq_model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "together_model = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\")\n",
        "openrouter_model = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def append_to_csv(new_row):\n",
        "    df = pd.read_csv(\"llm_comparison.csv\")\n",
        "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    df.to_csv(\"llm_comparison.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "response = nim_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"NVIDIA\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "response = groq_model_fast.invoke(messages)\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = response.response_metadata[\"token_usage\"][\"completion_time\"]\n",
        "new_row = {\n",
        "    \"provider\": \"Groq\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "response = groq_model.invoke(messages)\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = response.response_metadata[\"token_usage\"][\"completion_time\"]\n",
        "new_row = {\n",
        "    \"provider\": \"Groq\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "start_time = time.time()\n",
        "response = together_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"Together\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "start_time = time.time()\n",
        "response = openrouter_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"OpenRouter\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "from langchain_ollama import ChatOllama\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Local models\n",
        "ollama_model = ChatOllama(model=\"llama3.3:70b\")\n",
        "\n",
        "\n",
        "model_id = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "huggingface_model = ChatHuggingFace(llm=hf_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = ollama_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"eval_count\"]\n",
        "completion_time = response.response_metadata[\"total_duration\"] / 10**9\n",
        "new_row = {\n",
        "    \"provider\": \"Ollama\",\n",
        "    \"model\": response.response_metadata[\"model\"],\n",
        "    \"type\": \"local\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = len(tokenizer.encode(response.content))\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"HuggingFace\",\n",
        "    \"model\": model_id,\n",
        "    \"type\": \"local\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_message = \"Hello\"\n",
        "messages = [(\"user\", user_message)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "response = nim_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Hello! How can I assist you today?' additional_kwargs={} response_metadata={'role': 'assistant', 'content': 'Hello! How can I assist you today?', 'token_usage': {'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens': 9}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.3-70b-instruct'} id='run-c96d0c1d-8b48-4ad9-9d64-4907f217abdb-0' usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20} role='assistant'\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "response = groq_model_fast.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Hello. How can I assist you today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 35, 'total_tokens': 45, 'completion_time': 0.003883119, 'prompt_time': 0.004241874, 'queue_time': 0.044123721, 'total_time': 0.008124993}, 'model_name': 'llama-3.3-70b-specdec', 'system_fingerprint': 'fp_9eb2d06c09', 'finish_reason': 'stop', 'logprobs': None} id='run-4f87a7a8-ebde-4dbd-8d54-6215d088920a-0' usage_metadata={'input_tokens': 35, 'output_tokens': 10, 'total_tokens': 45}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "response = groq_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 36, 'total_tokens': 61, 'completion_time': 0.090909091, 'prompt_time': 0.00715968, 'queue_time': 0.01075391, 'total_time': 0.098068771}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_fcc3b74982', 'finish_reason': 'stop', 'logprobs': None} id='run-e2a1ab68-e090-4052-9f81-fa4cc74d8a9c-0' usage_metadata={'input_tokens': 36, 'output_tokens': 25, 'total_tokens': 61}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "response = together_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 36, 'total_tokens': 61, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None} id='run-5ac38395-b451-4b37-aef0-3ab8f2bdd7c8-0' usage_metadata={'input_tokens': 36, 'output_tokens': 25, 'total_tokens': 61, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "response = openrouter_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={'refusal': ''} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 11, 'total_tokens': 35, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/llama-3.3-70b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-c993e354-bee3-4758-9c05-184814d5025e-0' usage_metadata={'input_tokens': 11, 'output_tokens': 24, 'total_tokens': 35, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "response = ollama_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
