{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_message = \"\"\"\n",
        "Write a biography of Elon Musk across different time periods and fields he involed the most significantly.\n",
        "\"\"\"\n",
        "\n",
        "messages = [(\"user\", user_message)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teron/miniconda3/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found meta/llama-3.3-70b-instruct in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_together import ChatTogether\n",
        "\n",
        "# Cloud models\n",
        "nim_model = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\")\n",
        "groq_model_fast = ChatGroq(model=\"llama-3.3-70b-specdec\")\n",
        "groq_model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "together_model = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\")\n",
        "openrouter_model = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def append_to_csv(new_row):\n",
        "    df = pd.read_csv(\"llm_comparison.csv\")\n",
        "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    df.to_csv(\"llm_comparison.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "response = nim_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"NVIDIA\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "response = groq_model_fast.invoke(messages)\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = response.response_metadata[\"token_usage\"][\"completion_time\"]\n",
        "new_row = {\n",
        "    \"provider\": \"Groq\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "response = groq_model.invoke(messages)\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = response.response_metadata[\"token_usage\"][\"completion_time\"]\n",
        "new_row = {\n",
        "    \"provider\": \"Groq\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "start_time = time.time()\n",
        "response = together_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"Together\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "start_time = time.time()\n",
        "response = openrouter_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"token_usage\"][\"completion_tokens\"]\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"OpenRouter\",\n",
        "    \"model\": response.response_metadata[\"model_name\"],\n",
        "    \"type\": \"cloud\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "from langchain_ollama import ChatOllama\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Local models\n",
        "ollama_model = ChatOllama(model=\"llama3.3:70b\")\n",
        "\n",
        "\n",
        "model_id = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "huggingface_model = ChatHuggingFace(llm=hf_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = ollama_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = response.response_metadata[\"eval_count\"]\n",
        "completion_time = response.response_metadata[\"total_duration\"] / 10**9\n",
        "new_row = {\n",
        "    \"provider\": \"Ollama\",\n",
        "    \"model\": response.response_metadata[\"model\"],\n",
        "    \"type\": \"local\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "completion_tokens = len(tokenizer.encode(response.content))\n",
        "completion_time = end_time - start_time\n",
        "new_row = {\n",
        "    \"provider\": \"HuggingFace\",\n",
        "    \"model\": model_id,\n",
        "    \"type\": \"local\",\n",
        "    \"input\": user_message,\n",
        "    \"output\": response.content,\n",
        "    \"completion_tokens\": completion_tokens,\n",
        "    \"completion_time\": completion_time,\n",
        "    \"speed\": completion_tokens / completion_time,\n",
        "}\n",
        "append_to_csv(new_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_message = \"Hello\"\n",
        "messages = [(\"user\", user_message)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = nim_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = groq_model_fast.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = groq_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = together_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openrouter_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "response = ollama_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
