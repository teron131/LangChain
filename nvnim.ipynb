{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_message = \"Write a limerick about the wonders of GPU computing.\"\n",
        "messages = [(\"user\", user_message)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teron/miniconda3/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found meta/llama-3.3-70b-instruct in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_together import ChatTogether\n",
        "\n",
        "nim_model = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\")\n",
        "groq_model_fast = ChatGroq(model=\"llama-3.3-70b-specdec\")\n",
        "groq_model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "together_model = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\")\n",
        "openrouter_model = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "ollama_model = ChatOllama(model=\"llama3.3:70b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c833c1c03515455eac1e62429dddd4e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "model_id = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "huggingface_model = ChatHuggingFace(llm=hf_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_tokens(response):\n",
        "    return response.response_metadata.get(\"token_usage\", {}).get(\"completion_tokens\") or len(tokenizer.encode(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose parallel processing was divine,\n",
            "It crunched with great zest,\n",
            "Through its cores, it did quest,\n",
            "For solutions to problems of design.\n"
          ]
        }
      ],
      "source": [
        "response = nim_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose parallel processing was divine,\\nIt crunched with great zest,\\nThrough its cores, it did quest,\\nFor solutions to problems of design.' additional_kwargs={} response_metadata={'role': 'assistant', 'content': 'There once was a GPU so fine,\\nWhose parallel processing was divine,\\nIt crunched with great zest,\\nThrough its cores, it did quest,\\nFor solutions to problems of design.', 'token_usage': {'prompt_tokens': 22, 'total_tokens': 59, 'completion_tokens': 37}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.3-70b-instruct'} id='run-71e9f704-c32e-4bf9-b2f1-159cc66aacb7-0' usage_metadata={'input_tokens': 22, 'output_tokens': 37, 'total_tokens': 59} role='assistant'\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose computing powers did shine.\n",
            "It processed with pace,\n",
            "And a parallel face,\n",
            "Making complex tasks truly divine.\n"
          ]
        }
      ],
      "source": [
        "response = groq_model_fast.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did shine.\\nIt processed with pace,\\nAnd a parallel face,\\nMaking complex tasks truly divine.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 46, 'total_tokens': 78, 'completion_time': 0.028891941, 'prompt_time': 0.005572687, 'queue_time': 0.014289119999999999, 'total_time': 0.034464628}, 'model_name': 'llama-3.3-70b-specdec', 'system_fingerprint': 'fp_9eb2d06c09', 'finish_reason': 'stop', 'logprobs': None} id='run-b610ae88-11f4-4478-af11-e5c2ef138e05-0' usage_metadata={'input_tokens': 46, 'output_tokens': 32, 'total_tokens': 78}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once were GPUs so fine,\n",
            "Whose parallel processing did shine.\n",
            "They crunched with great speed,\n",
            "Through data with ease and deed,\n",
            "And made complex tasks truly divine.\n"
          ]
        }
      ],
      "source": [
        "response = groq_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once were GPUs so fine,\\nWhose parallel processing did shine.\\nThey crunched with great speed,\\nThrough data with ease and deed,\\nAnd made complex tasks truly divine.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83, 'completion_time': 0.130909091, 'prompt_time': 0.008215492, 'queue_time': 0.013799417999999999, 'total_time': 0.139124583}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c0cfa69934', 'finish_reason': 'stop', 'logprobs': None} id='run-7bb45ead-bd14-4ce4-8385-1b148ac737c3-0' usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose computing powers did shine.\n",
            "It processed with speed,\n",
            "And its parallel deed,\n",
            " Made complex tasks truly divine.\n"
          ]
        }
      ],
      "source": [
        "response = together_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did shine.\\nIt processed with speed,\\nAnd its parallel deed,\\n Made complex tasks truly divine.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 47, 'total_tokens': 79, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None} id='run-6be594c7-cb6c-4277-9417-84c03813695e-0' usage_metadata={'input_tokens': 47, 'output_tokens': 32, 'total_tokens': 79, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose computing powers did shine.\n",
            "It processed with speed,\n",
            "And its cores did proceed,\n",
            "To solve complex tasks in no time!\n"
          ]
        }
      ],
      "source": [
        "response = openrouter_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did shine.\\nIt processed with speed,\\nAnd its cores did proceed,\\nTo solve complex tasks in no time!' additional_kwargs={'refusal': ''} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 22, 'total_tokens': 56, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/llama-3.3-70b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9a0086a5-dafa-46e8-bfdf-605270fc720a-0' usage_metadata={'input_tokens': 22, 'output_tokens': 34, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There once was a GPU so fine,\n",
            "Whose parallel processing did shine.\n",
            "It crunched with great pace,\n",
            "And a wonderful face,\n",
            "And made computations divine.\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "response = ollama_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose parallel processing did shine.\\nIt crunched with great pace,\\nAnd a wonderful face,\\nAnd made computations divine.' additional_kwargs={} response_metadata={'model': 'llama3.3:70b', 'created_at': '2024-12-13T08:55:46.60147788Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2811016880, 'load_duration': 40834609, 'prompt_eval_count': 22, 'prompt_eval_duration': 87273000, 'eval_count': 33, 'eval_duration': 2640163000} id='run-d08c1855-1daa-4e40-9006-f7e150e0f2d2-0' usage_metadata={'input_tokens': 22, 'output_tokens': 33, 'total_tokens': 55}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken: 16.20 seconds\n",
            "There once was a GPU so fine,\n",
            "Whose computing powers did truly shine.\n",
            "It processed with zest,\n",
            "And its speeds were the best,\n",
            "And its parallel tasks did align.\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "start_time = time.time()\n",
        "response = huggingface_model.invoke(messages)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='There once was a GPU so fine,\\nWhose computing powers did truly shine.\\nIt processed with zest,\\nAnd its speeds were the best,\\nAnd its parallel tasks did align.' additional_kwargs={} response_metadata={} id='run-c6c15db5-d113-4f40-88d9-5b72c1e00fce-0'\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
